{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6911fd0a-f363-4e50-b074-621264c0a46e",
   "metadata": {},
   "source": [
    "![anh1](https://scontent.fhan18-1.fna.fbcdn.net/v/t39.30808-6/527568808_784899461143393_6790818195720877561_n.png?_nc_cat=110&ccb=1-7&_nc_sid=cc71e4&_nc_eui2=AeHP5E0E4DEEVLZ8-gwDOgWYecLcIH_fR7d5wtwgf99Ht2HgvQ0PuYjK_Yi9Tkp7KE8FuUHphBRToDfSxb5URMG5&_nc_ohc=8hkkWWc7tNUQ7kNvwFCAiBN&_nc_oc=Adl0d-l6hr2sOc_3Y1d4a43ZJJ27w73j3w3E_28uI5ubp-gVt5k2PibqVQeIzHtqXgs&_nc_zt=23&_nc_ht=scontent.fhan18-1.fna&_nc_gid=JaJRkErUCv85lJUT14WBRw&oh=00_AfavYKIS4YqSQtjbZVHRHlQ7ln3H7GDiAXp9-LUUOliKww&oe=68BF98AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbce87c-b34d-459e-9417-45a9fb73493f",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Bài làm vòng 2: Xây dựng một chiến lược lựa chọn cổ phiếu tiềm năng sinh lời</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fc2a0-b542-4de1-8301-21b9e24ac5ce",
   "metadata": {},
   "source": [
    "<center><h4>Nhóm Votri:</h4></center>\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Thành viên</th>\n",
    "            <th>Họ và tên</th>\n",
    "            <th>Số báo danh</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>1.</td>\n",
    "            <td>Trương Thị Thùy Dung</td>\n",
    "            <td>B0426</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2.</td>\n",
    "            <td>Nguyễn Thế Anh</td>\n",
    "            <td>B0396</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3.</td>\n",
    "            <td>Trần Mạnh Hùng</td>\n",
    "            <td>B0535</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccc891-2273-49ac-8c19-501ba8b9befd",
   "metadata": {},
   "source": [
    "*Logic cốt lõi của chiến lược là dự báo xác suất thành công của một cơ hội giao dịch dựa trên một kiến trúc Deep Learning tổng hợp, cụ thể \n",
    "là CNN + LSTM + Multi-Head Attention với đầu vào là chỉ số kĩ thuật (TA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd56ef5-a168-4911-873c-32e273f18f39",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h2>CHUẨN BỊ</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cee61e-ffe2-40c7-8a21-3ca90255ef26",
   "metadata": {},
   "source": [
    "Nhập, cấu hình các môi trường, thư viện cần thiết:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135da758-2fb5-4b5e-af17-1ec7ee4592e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# ML libs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# optional SOM\n",
    "try:\n",
    "    from minisom import MiniSom\n",
    "    HAS_SOM = True\n",
    "except Exception:\n",
    "    HAS_SOM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a71442-4e3a-4df7-be67-64c95fd6cef0",
   "metadata": {},
   "source": [
    "Đăng nhập tài khoản và sử dụng FiinQuantX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fc5ae8-fbb1-4d2d-8d1f-4c48da249cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FiinQuantX import FiinSession\n",
    "\n",
    "username = 'DSTC_36@fiinquant.vn'\n",
    "password = 'Fiinquant0606'\n",
    "\n",
    "client = FiinSession(\n",
    "    username=username,\n",
    "    password=password,\n",
    ").login()\n",
    "\n",
    "fi = client.FiinIndicator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34927ed4-0a62-48c0-adf1-eee3f6adc164",
   "metadata": {},
   "source": [
    "Cấu hình\n",
    "- GPU để chạy TensorFlow\n",
    "- mô hình Deep Learning với Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e4f929-fce0-4d44-9fcf-6ba5827ddb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected - using CPU\n",
      "Available RAM: 3.1 GB\n"
     ]
    }
   ],
   "source": [
    "# TF / Keras\n",
    "import tensorflow as tf\n",
    "# Configure GPU before any other TF operation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Set GPU as preferred device\n",
    "        tf.config.experimental.set_device_policy('explicit')\n",
    "        print(f\"GPU configured successfully. Available GPUs: {len(gpus)}\")\n",
    "        \n",
    "        # Force specific GPU if multiple available\n",
    "        with tf.device('/GPU:0'):\n",
    "            tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        print(\"GPU test successful - using GPU for computation\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "        print(\"Falling back to CPU\")\n",
    "else:\n",
    "    print(\"No GPU detected - using CPU\")\n",
    "\n",
    "# Memory optimization\n",
    "import psutil\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Dropout, LSTM,\n",
    "                                     LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D,\n",
    "                                     Dense, Embedding, Flatten, Concatenate)\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88387a-7e19-4d17-a6a7-a2d82bc4db10",
   "metadata": {},
   "source": [
    "Sử dụng matplotlib để vẽ đồ thị:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1a1cf4-9a1d-4d7a-a772-a245eab554c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee06e84-c456-4cb1-b1c9-aa9788898c22",
   "metadata": {},
   "source": [
    "Cấu hình và thông số cho pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03d82c9-01ed-4110-b193-dd35a5a11366",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"DSTC_3y.csv\"   # Đường dẫn đến file dữ liệu chứa dữ liệu lịch sử của các cổ phiếu.\n",
    "START_DATE = None        # Bắt đầu thời gian để lọc data (None thì giữ hết)\n",
    "END_DATE = None          # Kết thúc thời gian để lọc data\n",
    "RF_ANNUAL = 0.03          # Tỉ lệ lợi nhuận hàng năm dự tính\n",
    "MAX_MISSING_RATIO = 0.10          # Tỉ lệ dữ liệu thiếu tối đa cho phép trong các cột dữ liệu, nếu vượt quá sẽ bị loại bỏ\n",
    "N_RFE_FEATURES = 40          # Số lượng features chọn lựa bằng phương pháp RFE\n",
    "N_CLUSTERS = 6           # Số clusters khi áp dụng KMeans\n",
    "FINAL_FEATURE_COUNT = 20           # Số lượng đặc trưng cuối cùng sau khi lựa chọn và lọc các đặc trưng\n",
    "WINDOW = 40           # Kích thước window cho mô hình deep learning\n",
    "STEP = 1             # Bước nhảy khi xây dựng các chuỗi dữ liệu\n",
    "RFE_ESTIMATORS = 100     # Số lượng ước lượng trong RandomForest dùng cho RFE\n",
    "SOM_ITERS = 200         # Số lần lặp cho Self-Organizing Maps (SOM)\n",
    "EPOCHS = 100              # Số lượng epochs (vòng lặp) khi huấn luyện mô hình\n",
    "BATCH_SIZE = 64              # Kích thước của mỗi batch khi huấn luyện mô hình\n",
    "EMB_DIM = 16              # Kích thước embedding cho mô hình\n",
    "OUT_DIR = \"outputs\"                # Thư mục chứa kết quả xuất ra (outputs)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "TS = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")               # Chuỗi thời gian (timestamp) duy nhất để đặt tên cho các kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f4f84-6440-482c-bb3c-f1945b85df7a",
   "metadata": {},
   "source": [
    "Setting cho phương pháp Triple-Barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2790150-12df-455c-a9f0-c619cb730e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKE_PROFIT_MULT = 2   # Rào cản chốt lời = 3 * ATR (R:R ~ 1:2)\n",
    "STOP_LOSS_MULT = 1   # Rào cản cắt lỗ = 1 * ATR\n",
    "MAX_HOLD_PERIOD = 7   # Giữ vị thế tối đa 7 ngày"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60ee17-f1b8-4b19-9aee-f73e66080a10",
   "metadata": {},
   "source": [
    "Setting xử lý data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee61e734-9d06-404f-9b10-a6be8c2bc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 10000       # Số lượng samples tối đa trong mỗi phần dữ liệu khi tải vào bộ nhớ\n",
    "MAX_SAMPLES = 50000      # Giới hạn số lượng samples sẽ được sử dụng trong training để tránh quá tải bộ nhớ OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f1564-be80-4233-a5f9-e4208704c9ba",
   "metadata": {},
   "source": [
    "## Khai báo các hàm cần thiết cho chương trình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ff27-3e15-47b2-9afd-ba639c537938",
   "metadata": {},
   "source": [
    "Hàm `check_memory()` theo dõi việc sử dụng bộ nhớ và đảm bảo không quá tải bộ nhớ trong quá trình xử lý dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684ce5f7-0217-449c-9411-cec5a1f7e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    \"\"\"Monitor memory usage\"\"\"\n",
    "    mem = psutil.virtual_memory()  # Lấy thông tin bộ nhớ hệ thống\n",
    "    print(f\"Memory usage: {mem.percent:.1f}% ({(mem.total - mem.available)/(1024**3):.1f}/{mem.total/(1024**3):.1f} GB)\")  \n",
    "    # In ra tỉ lệ sử dụng bộ nhớ và tổng bộ nhớ, bộ nhớ còn lại\n",
    "    if mem.percent > 85:  # Nếu sử dụng bộ nhớ trên 85%\n",
    "        print(\"WARNING: High memory usage detected!\")  # Cảnh báo sử dụng bộ nhớ cao\n",
    "        gc.collect()  # Dọn dẹp bộ nhớ (garbage collection)\n",
    "        return True  # Trả về True khi bộ nhớ cao\n",
    "    return False  # Trả về False nếu bộ nhớ dưới mức cảnh báo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1dc1c9-d5a9-42f5-9786-f340ae088a47",
   "metadata": {},
   "source": [
    "Hàm `load_data()` đọc dữ liệu từ tệp CSV, phân mảnh để quản lý bộ nhớ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fdfee7e-f174-4ed5-8dd1-b76f2e158444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    if not os.path.exists(path):  # Kiểm tra xem file có tồn tại không\n",
    "        raise FileNotFoundError(path)  # Nếu không có file, raise lỗi\n",
    "\n",
    "    # Load in chunks to manage memory\n",
    "    print(\"Loading data in chunks...\")  # In ra thông báo đang tải dữ liệu theo từng phần\n",
    "    chunk_list = []  # Danh sách lưu các phần dữ liệu (chunks)\n",
    "    \n",
    "    try:\n",
    "        for chunk in pd.read_csv(path, chunksize=CHUNK_SIZE):  # Đọc dữ liệu từ file theo từng phần\n",
    "            if 'timestamp' in chunk.columns:  # Kiểm tra xem cột 'timestamp' có trong dữ liệu không\n",
    "                chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])  # Chuyển đổi cột 'timestamp' sang kiểu datetime\n",
    "            chunk_list.append(chunk)  # Thêm phần dữ liệu vào danh sách\n",
    "            \n",
    "            # Kiểm tra bộ nhớ sau khi đọc mỗi phần\n",
    "            if check_memory():  # Nếu bộ nhớ cao, dừng việc đọc thêm dữ liệu\n",
    "                print(\"Memory pressure detected, reducing chunk size...\")\n",
    "                break  # Dừng vòng lặp nếu bộ nhớ quá tải\n",
    "                \n",
    "        df = pd.concat(chunk_list, ignore_index=True)  # Kết hợp tất cả các phần dữ liệu thành một DataFrame\n",
    "        del chunk_list  # Xóa chunk_list để giải phóng bộ nhớ\n",
    "        gc.collect()  # Dọn dẹp bộ nhớ sau khi kết hợp dữ liệu\n",
    "        \n",
    "        # Giới hạn số lượng mẫu nếu quá nhiều\n",
    "        if len(df) > MAX_SAMPLES:  # Nếu số lượng mẫu vượt quá giới hạn MAX_SAMPLES\n",
    "            print(f\"Limiting dataset from {len(df)} to {MAX_SAMPLES} samples\")  # Thông báo sẽ giới hạn số mẫu\n",
    "            df = df.sample(n=MAX_SAMPLES, random_state=42).sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
    "            # Giới hạn số mẫu và sắp xếp lại theo 'ticker' và 'timestamp'\n",
    "            \n",
    "        print(f\"Data loaded: {df.shape}\")  # In ra kích thước dữ liệu đã tải\n",
    "        return df  # Trả về DataFrame chứa dữ liệu đã tải\n",
    "        \n",
    "    except Exception as e:  # Nếu có lỗi khi đọc dữ liệu\n",
    "        print(f\"Error loading data: {e}\")  # In ra lỗi\n",
    "        # Thử tải một phần dữ liệu nhỏ hơn\n",
    "        print(\"Attempting to load smaller subset...\")\n",
    "        df = pd.read_csv(path, nrows=MAX_SAMPLES//2)  # Đọc một nửa số mẫu nếu có lỗi\n",
    "        if 'timestamp' in df.columns:  # Kiểm tra lại và chuyển đổi cột 'timestamp'\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        return df  # Trả về DataFrame đã tải một phần"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47663131-a9e4-4d7d-ab94-a3015e49c39b",
   "metadata": {},
   "source": [
    "Hàm `drop_high_missing()` loại bỏ các cột trong DataFrame có tỉ lệ missing value vượt quá mức quy định (max_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aaadfe6-83bf-489b-93a4-4c60f730e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_missing(df, max_ratio):\n",
    "    missing = df.isna().mean()  # Tính toán tỉ lệ missing value (giá trị thiếu) trên mỗi cột\n",
    "    drop_cols = missing[missing > max_ratio].index.tolist()  # Chọn các cột có tỉ lệ missing value vượt quá max_ratio\n",
    "    if drop_cols:  # Nếu có cột nào có missing > max_ratio, tiến hành loại bỏ\n",
    "        print(f\"Dropping {len(drop_cols)} columns with >{max_ratio*100:.1f}% missing\")\n",
    "        df = df.drop(columns=drop_cols)  # Loại bỏ các cột có missing value cao\n",
    "    return df  # Trả về DataFrame đã loại bỏ các cột không cần thiết"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf4697-cdca-48d2-b6ee-1a11145639fe",
   "metadata": {},
   "source": [
    "Hàm `interp_per_ticker()` thay thế các giá trị thiếu trong các cột số (numeric_cols) bằng phương pháp nội suy (interpolation), và nội suy được thực hiện theo từng nhóm ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7078cf2a-6795-47a6-86af-c59d3fcd8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_per_ticker(df, numeric_cols):\n",
    "    # interpolate per ticker to avoid leaking across tickers\n",
    "    for t, g in df.groupby('ticker'):  # Nhóm dữ liệu theo từng 'ticker'\n",
    "        idx = g.index  # Lấy chỉ số của nhóm\n",
    "        df.loc[idx, numeric_cols] = g[numeric_cols].interpolate(method='linear', limit_direction='both').ffill().bfill().values\n",
    "        # Sử dụng phương pháp nội suy (interpolation) tuyến tính để thay thế giá trị thiếu trong các cột numeric_cols\n",
    "    return df  # Trả về DataFrame đã được nội suy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d60f59-7c01-4b4d-b826-b7db21484f6b",
   "metadata": {},
   "source": [
    "#### Về Triple-Barrier Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189f6c2-705d-4ae9-85da-38628a509126",
   "metadata": {},
   "source": [
    "Phương pháp Triple-Barrier bao gồm ba yếu tố quan trọng:\n",
    "- Take Profit (TP): Mức giá khi giá đạt đến sẽ chốt lời\n",
    "  \n",
    "- Stop Loss (SL): Mức giá khi giá đạt đến sẽ cắt lỗ\n",
    "\n",
    "  \n",
    "- Timeout: Thời gian tối đa được phép giữ vị thế (trade), nếu không đạt TP hoặc SL trong khoảng thời gian này, thì coi như hết thời gian\n",
    "\n",
    "Phương pháp Triple-Barrier để tạo nhãn (labels) cho mỗi cổ phiếu trong dữ liệu, dùng để phân loại các tín hiệu mua bán. Mục đích chính là xác định khi nào chốt lời (take profit), cắt lỗ (stop loss), hoặc hết thời gian giữ (timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b300e-45eb-4bb1-9609-ce86c52e18b8",
   "metadata": {},
   "source": [
    "Hàm `apply_triple_barrier()` thực hiện phương pháp **Triple-Barrier** để tạo nhãn cho mỗi cổ phiếu trong dữ liệu. Mỗi nhãn có thể là:\n",
    "+ 1 (win): Nếu mức chốt lời (take profit) được đạt\n",
    "  \n",
    "+ 0 (loss): Nếu mức cắt lỗ (stop loss) được đạt\n",
    "\n",
    "+ 2 (timeout): Nếu hết thời gian giữ vị thế mà không đạt cả TP và SL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11be8787-14bb-4f02-80a3-e0d1be057159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_triple_barrier(df, tp_mult=1.5, sl_mult=1.5, max_period=10):\n",
    "    \"\"\"\n",
    "    Applies the Triple-Barrier Method to generate labels for each ticker.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing price data and 'atr_14'.\n",
    "        tp_mult (float): Multiplier for ATR to set the take-profit barrier.\n",
    "        sl_mult (float): Multiplier for ATR to set the stop-loss barrier.\n",
    "        max_period (int): Maximum number of days to hold the position.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series with labels (1 for win, 0 for loss, 2 for timeout).\n",
    "    \"\"\"\n",
    "    print(f\"Applying Triple-Barrier with TP={tp_mult}*ATR, SL={sl_mult}*ATR, Max Hold={max_period} days...\")\n",
    "    \n",
    "    # Create an empty Series to store the results, ensuring it can handle NaNs before filling\n",
    "    all_labels = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    # Process each ticker individually\n",
    "    for ticker, group in df.groupby('ticker'):\n",
    "        group = group.sort_index() # Ensure data is sorted by time\n",
    "        labels = pd.Series(np.nan, index=group.index)\n",
    "        \n",
    "        # Iterate through each time point to start a \"trade\"\n",
    "        for i in range(len(group) - 1):\n",
    "            entry_idx = group.index[i]\n",
    "            entry_price = group.loc[entry_idx, 'close']\n",
    "            atr = group.loc[entry_idx, 'atr_14']\n",
    "\n",
    "            # Skip if ATR is missing or zero\n",
    "            if pd.isna(atr) or atr == 0:\n",
    "                continue\n",
    "\n",
    "            # 1. Define the barriers\n",
    "            upper_barrier = entry_price + (atr * tp_mult)\n",
    "            lower_barrier = entry_price - (atr * sl_mult)\n",
    "\n",
    "            # 2. Define the time window\n",
    "            window_end_idx = min(i + max_period, len(group) - 1)\n",
    "            window_indices = group.index[i+1 : window_end_idx+1]\n",
    "            price_window = group.loc[window_indices]\n",
    "\n",
    "            # 3. Find the time when barriers are hit\n",
    "            hit_tp = price_window[price_window['high'] >= upper_barrier]\n",
    "            hit_sl = price_window[price_window['low'] <= lower_barrier]\n",
    "\n",
    "            time_to_tp = hit_tp.index.min() if not hit_tp.empty else pd.NaT\n",
    "            time_to_sl = hit_sl.index.min() if not hit_sl.empty else pd.NaT\n",
    "            \n",
    "            # 4. Determine the label\n",
    "            # Case 1: Both barriers are hit\n",
    "            if pd.notna(time_to_tp) and pd.notna(time_to_sl):\n",
    "                # Choose the one that was hit first\n",
    "                if time_to_tp < time_to_sl:\n",
    "                    labels.loc[entry_idx] = 1 # Win\n",
    "                else:\n",
    "                    labels.loc[entry_idx] = 0 # Loss\n",
    "            # Case 2: Only Take Profit is hit\n",
    "            elif pd.notna(time_to_tp):\n",
    "                labels.loc[entry_idx] = 1 # Win\n",
    "            # Case 3: Only Stop Loss is hit\n",
    "            elif pd.notna(time_to_sl):\n",
    "                labels.loc[entry_idx] = 0 # Loss\n",
    "            # Case 4: No barrier is hit (Timeout)\n",
    "            else:\n",
    "                labels.loc[entry_idx] = 2 # Timeout\n",
    "\n",
    "        all_labels.update(labels)\n",
    "    \n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480772de-74f8-449f-abc7-099901803290",
   "metadata": {},
   "source": [
    "Hàm `time_split_train_mask()` chia dữ liệu theo thời gian dựa trên cột `timestamp`, và tạo ra một mask (chuỗi boolean) để xác định những mẫu nào sẽ được dùng cho huấn luyện và kiểm tra. Tỉ lệ dữ liệu huấn luyện được xác định bởi tham số `train_frac`, và dữ liệu được chia theo thời gian, không gian (random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051cc410-9eac-4982-9730-a1d38142ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split_train_mask(df, train_frac=0.70):\n",
    "    # compute global train end date by unique sorted dates\n",
    "    if 'timestamp' not in df.columns:\n",
    "        raise RuntimeError('timestamp column required for time split')\n",
    "    unique_days = np.sort(df['timestamp'].dt.floor('D').unique())\n",
    "    if len(unique_days) < 3:\n",
    "        raise RuntimeError('Too few unique days to split')\n",
    "    train_end = unique_days[int(len(unique_days)*train_frac)-1]\n",
    "    mask = df['timestamp'].dt.floor('D') <= train_end\n",
    "    print(f\"Time-split train_end date: {train_end} -> train rows: {mask.sum()} / {len(df)}\")\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb8873-22e5-4493-bda4-287026d31a9e",
   "metadata": {},
   "source": [
    "Hàm `train_only_rfe()` thực hiện việc chọn lọc các đặc trưng quan trọng từ dữ liệu bằng phương pháp **RFE** với mô hình **RandomForestClassifier**. Nó giúp loại bỏ các đặc trưng không cần thiết và chỉ giữ lại những đặc trưng quan trọng, có ảnh hưởng lớn đến dự đoán. Kết quả là một danh sách các đặc trưng quan trọng cùng với mô hình **RFE** đã huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e15e50-3ca5-463b-8f79-f2507c991ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_only_rfe(df, numeric_cols, train_mask, n_select=64, n_estimators=100):\n",
    "    X = df.loc[train_mask, numeric_cols].fillna(0.0).values\n",
    "    y = df.loc[train_mask, 'target'].values\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    n_select = min(n_select, X.shape[1])\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_select, step=1)\n",
    "    print(\"Fitting RFE on train rows... (this may take time)\")\n",
    "    rfe.fit(X, y)\n",
    "    feature_ranking = sorted(zip(rfe.ranking_, numeric_cols))\n",
    "    print(\"\\nFeature Ranking by RFE:\")\n",
    "    for rank, name in feature_ranking:\n",
    "        print(f\"Rank {rank}: {name}\")\n",
    "    keep = [c for c, s in zip(numeric_cols, rfe.support_) if s]\n",
    "    return keep, rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b782b-c036-4ca7-bb87-f9e90c7672c3",
   "metadata": {},
   "source": [
    "Hàm `kmeans_cluster_features()` sử dụng thuật toán **KMeans** để phân nhóm các đặc trưng của dữ liệu huấn luyện thành các cụm (clusters). Việc phân nhóm này có thể giúp trong việc lựa chọn các đặc trưng quan trọng hoặc giảm chiều dữ liệu. Hàm trả về nhãn phân nhóm của các đặc trưng, đối tượng `scaler` để chuẩn hóa dữ liệu và đối tượng k để tra cứu thêm thông tin về mô hình **KMeans**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646e5837-522b-4d1d-9c87-de29a0fe4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster_features(X_train_selected, n_clusters=5):\n",
    "    # X_train_selected: DataFrame of selected features (train rows only)\n",
    "    # Chuẩn hóa dữ liệu:\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X_train_selected.values)\n",
    "    # cluster across features -> transpose\n",
    "    k = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = k.fit_predict(Xs.T)\n",
    "    return labels, scaler, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8ce2a-3c16-4714-b29a-b7f1850524a0",
   "metadata": {},
   "source": [
    "Hàm `som_select_per_cluster()` sử dụng **Self-Organizing Map (SOM)** (nếu có sẵn thư viện `MiniSom`) để chọn lọc các đặc trưng quan trọng từ mỗi nhóm (cluster). Nếu không sử dụng SOM, nó sẽ chọn các đặc trưng dựa trên độ tương quan giữa các đặc trưng. Cuối cùng, hàm trả về danh sách các đặc trưng được chọn từ mỗi nhóm, giúp giảm số lượng đặc trưng mà vẫn đảm bảo tính hiệu quả trong mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace21975-e2a0-4980-82a7-6fd690c55663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def som_select_per_cluster(X_train_selected, labels_cluster, feature_names, target_count=10):\n",
    "    # X_train_selected: DataFrame (train_rows x features)\n",
    "    chosen = []\n",
    "    features = np.array(feature_names)  # Chuyển tên feature thành mảng numpy\n",
    "    for cl in np.unique(labels_cluster):  # Lặp qua từng nhóm phân loại\n",
    "        idxs = np.where(labels_cluster == cl)[0]  # Tìm các chỉ số (index) thuộc nhóm hiện tại\n",
    "        if len(idxs) == 0:  # Nếu không có feature nào thuộc nhóm, bỏ qua\n",
    "            continue\n",
    "        feats = features[idxs]  # Lấy các feature thuộc nhóm\n",
    "        cluster_data = X_train_selected[feats].values  # Dữ liệu của các feature trong nhóm\n",
    "        # transpose -> (n_feats, n_train_rows)\n",
    "        v = cluster_data.T  # Chuyển vị dữ liệu: đặc trưng -> mẫu\n",
    "        eps = 1e-8  # Hệ số nhỏ để tránh chia cho 0\n",
    "        v = (v - v.mean(axis=1, keepdims=True)) / (v.std(axis=1, keepdims=True) + eps)  # Chuẩn hóa từng feature\n",
    "        n_feats = v.shape[0]\n",
    "        som_x = max(1, int(math.sqrt(n_feats)))  # Tính toán kích thước lưới SOM\n",
    "        som_y = som_x\n",
    "        if HAS_SOM:  # Kiểm tra xem có thư viện MiniSom hay không\n",
    "            som = MiniSom(som_x, som_y, v.shape[1], sigma=0.5, learning_rate=0.5)  # Khởi tạo SOM\n",
    "            som.random_weights_init(v)  # Khởi tạo trọng số ngẫu nhiên\n",
    "            som.train_random(v, SOM_ITERS)  # Huấn luyện SOM\n",
    "            mapped = [som.winner(v_i) for v_i in v]  # Tìm vị trí thắng (winner) của mỗi đặc trưng\n",
    "            node_counts = Counter(mapped)  # Đếm số lượng đặc trưng được ánh xạ tới mỗi node\n",
    "            nodes_sorted = [n for n, _ in node_counts.most_common()]  # Sắp xếp các node theo số lượng đặc trưng được ánh xạ tới\n",
    "            picks = []  # Danh sách đặc trưng được chọn\n",
    "            for node in nodes_sorted:\n",
    "                idxs_node = [i for i, m in enumerate(mapped) if m == node]  # Các đặc trưng ánh xạ đến node hiện tại\n",
    "                for ii in idxs_node:\n",
    "                    f = feats[ii]\n",
    "                    if f not in picks:\n",
    "                        picks.append(f)  # Thêm đặc trưng vào danh sách nếu chưa có\n",
    "                    if len(picks) >= max(1, int(len(feats) * target_count / max(1, len(feature_names)))):  # Đảm bảo không vượt quá target_count\n",
    "                        break\n",
    "                if len(picks) >= max(1, int(len(feats) * target_count / max(1, len(feature_names)))):  # Dừng nếu đã đủ số lượng đặc trưng\n",
    "                    break\n",
    "            if len(picks) == 0:\n",
    "                # fallback choose top variance\n",
    "                var_idx = np.argsort(-np.var(v, axis=1))[:1]\n",
    "                picks = [feats[i] for i in var_idx]\n",
    "        else:\n",
    "            # fallback: chọn các đặc trưng có tương quan cao nhất\n",
    "            if v.shape[0] == 1:\n",
    "                picks = [feats[0]]\n",
    "            else:\n",
    "                corr = np.corrcoef(v)  # Tính ma trận tương quan\n",
    "                avg_abs = np.nanmean(np.abs(corr), axis=1)  # Tính trung bình giá trị tuyệt đối của tương quan\n",
    "                order = np.argsort(-avg_abs)  # Sắp xếp các đặc trưng theo tương quan\n",
    "                n_pick = max(1, int(len(feats) * target_count / max(1, len(feature_names))))  # Đảm bảo số lượng đặc trưng không vượt quá target_count\n",
    "                picks = [feats[i] for i in order[:n_pick]]  # Chọn các đặc trưng có tương quan cao nhất\n",
    "        chosen.extend(picks)\n",
    "    # dedupe preserving order\n",
    "    seen = set()  # Set dùng để kiểm tra các đặc trưng đã chọn\n",
    "    final = [x for x in chosen if not (x in seen or seen.add(x))]  # Loại bỏ các đặc trưng trùng lặp\n",
    "    return final[:target_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fae894-97e8-4c9b-98b5-aa5efb65f433",
   "metadata": {},
   "source": [
    "Hàm `correlation_filter()` kiểm tra tính tương quan giữa các đặc trưng trong `df_full`. Nếu hai đặc trưng có hệ số tương quan tuyệt đối vượt quá ngưỡng `threshold`, một trong hai đặc trưng sẽ bị loại bỏ. Kết quả là một danh sách các đặc trưng không có sự dư thừa về thông tin, giúp cải thiện hiệu suất của mô hình học máy và tránh hiện tượng **multicollinearity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54f38f6-a355-4ff6-ab7b-650cef4cab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_filter(df_full, features, threshold=0.8):\n",
    "    sub = df_full[features].corr().abs()\n",
    "    to_drop = set()\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i+1, len(features)):\n",
    "            if sub.iloc[i,j] > threshold:\n",
    "                # drop j (later) to keep earlier\n",
    "                to_drop.add(features[j])\n",
    "    filtered = [f for f in features if f not in to_drop]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ae597-61a9-40f5-b7a4-8f42452a994a",
   "metadata": {},
   "source": [
    "#### Hàm tính toán các chỉ số kỹ thuật (TA) và cơ bản (FA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755264ef-5940-4c47-a4e1-ece08f913a6a",
   "metadata": {},
   "source": [
    "Hàm `compute_indicators()` tính toán và thêm vào DataFrame các chỉ số kỹ thuật như *Moving Averages*, *RSI*, *ATR*, *MACD*, *Bollinger Bands*, *ADX*, *Ichimoku*, và các chỉ số khối lượng như *OBV* và *MFI*. Nó sử dụng thư viện **FiinQuantX** có sẵn và cả tự tính toán bằng **pandas**. Những chỉ số này rất quan trọng trong phân tích kỹ thuật và giúp xây dựng các mô hình học máy trong dự đoán giá cổ phiếu hoặc tài sản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e833c0-55cd-4ec2-b272-5f3617224a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indicators(df):\n",
    "    \"\"\"Compute a compact set of technical indicators used by the pipeline.\n",
    "    Adds columns in-place to df. Implementations are lightweight and avoid external APIs.\n",
    "    \"\"\"\n",
    "    # ensure timestamp\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # If a FiinQuantX indicator object exists in globals, prefer it\n",
    "    use_fi = 'fi' in globals() and globals().get('fi') is not None\n",
    "\n",
    "    # 1) Moving averages + WMA\n",
    "    ma_windows = [5, 10, 20, 50]\n",
    "    for w in ma_windows:\n",
    "        if use_fi:\n",
    "            try:\n",
    "                df[f'ema_{w}'] = fi.ema(df['close'], window=w)\n",
    "                df[f'sma_{w}'] = fi.sma(df['close'], window=w)\n",
    "                df[f'wma_{w}'] = fi.wma(df['close'], window=w)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        # pandas group-based EMAs/SMA\n",
    "        df[f'ema_{w}'] = df.groupby('ticker')['close'].transform(lambda x, p=w: x.ewm(span=p, adjust=False).mean())\n",
    "        df[f'sma_{w}'] = df.groupby('ticker')['close'].transform(lambda x, p=w: x.rolling(window=p, min_periods=1).mean())\n",
    "        # WMA implementation\n",
    "        def _wma(s, p=w):\n",
    "            weights = np.arange(1, p+1)\n",
    "            return s.rolling(p, min_periods=1).apply(lambda x: np.dot(x, weights[-len(x):]) / weights[-len(x):].sum(), raw=True)\n",
    "        df[f'wma_{w}'] = df.groupby('ticker')['close'].transform(lambda x, p=w: _wma(x, p))\n",
    "\n",
    "    # RSI\n",
    "    def _rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        up = delta.clip(lower=0)\n",
    "        down = -delta.clip(upper=0)\n",
    "        ma_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
    "        ma_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
    "        rs = ma_up / (ma_down + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    for w in [7, 14, 30]:\n",
    "        if use_fi:\n",
    "            try:\n",
    "                df[f'rsi_{w}'] = fi.rsi(df['close'], window=w)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        df[f'rsi_{w}'] = df.groupby('ticker')['close'].transform(lambda x, p=w: _rsi(x, p))\n",
    "\n",
    "    # ATR\n",
    "    def _atr(g, period=14):\n",
    "        high = g['high']; low = g['low']; close = g['close']\n",
    "        prev_close = close.shift(1)\n",
    "        tr = pd.concat([high - low, (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "        return tr.rolling(window=period, min_periods=1).mean()\n",
    "\n",
    "    for w in [7, 14, 21]:\n",
    "        if use_fi:\n",
    "            try:\n",
    "                df[f'atr_{w}'] = fi.atr(df['high'], df['low'], df['close'], window=w)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        df[f'atr_{w}'] = df.groupby('ticker').apply(lambda g, p=w: _atr(g, period=p)).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bb_windows = [10, 20, 30]\n",
    "    bb_devs = [1.5, 2.0, 2.5]\n",
    "    for w in bb_windows:\n",
    "        for d in bb_devs:\n",
    "            if use_fi:\n",
    "                try:\n",
    "                    df[f'bollinger_hband_{w}_{d}'] = fi.bollinger_hband(df['close'], window=w, window_dev=d)\n",
    "                    df[f'bollinger_lband_{w}_{d}'] = fi.bollinger_lband(df['close'], window=w, window_dev=d)\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "            m = df.groupby('ticker')['close'].transform(lambda x, p=w: x.rolling(window=p, min_periods=1).mean())\n",
    "            s = df.groupby('ticker')['close'].transform(lambda x, p=w: x.rolling(window=p, min_periods=1).std())\n",
    "            df[f'bollinger_hband_{w}_{d}'] = m + d * s\n",
    "            df[f'bollinger_lband_{w}_{d}'] = m - d * s\n",
    "\n",
    "    # MACD variants\n",
    "    def _ema(series, span):\n",
    "        return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    macd_params = [(12, 26, 9), (5, 35, 5), (20, 50, 10)]\n",
    "    for fast, slow, signal in macd_params:\n",
    "        name = f\"{fast}_{slow}_{signal}\"\n",
    "        if use_fi:\n",
    "            try:\n",
    "                df[f'macd_{name}'] = fi.macd(df['close'], window_fast=fast, window_slow=slow)\n",
    "                df[f'macd_signal_{name}'] = fi.macd_signal(df['close'], window_fast=fast, window_slow=slow, window_sign=signal)\n",
    "                df[f'macd_diff_{name}'] = fi.macd_diff(df['close'], window_fast=fast, window_slow=slow, window_sign=signal)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        df[f'macd_{name}'] = df.groupby('ticker')['close'].transform(lambda x, f=fast, s=slow: _ema(x, f) - _ema(x, s))\n",
    "        df[f'macd_signal_{name}'] = df.groupby('ticker')['macd_' + name].transform(lambda x, p=signal: _ema(x, p))\n",
    "        df[f'macd_diff_{name}'] = df[f'macd_{name}'] - df[f'macd_signal_{name}']\n",
    "\n",
    "    # Stochastic Oscillator and signal\n",
    "    for w in [10, 14, 20]:\n",
    "        if use_fi:\n",
    "            try:\n",
    "                df[f'stoch_{w}'] = fi.stoch(df['high'], df['low'], df['close'], window=w)\n",
    "                df[f'stoch_signal_{w}'] = fi.stoch_signal(df['high'], df['low'], df['close'], window=w, smooth_window=3)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        k = df.groupby('ticker').apply(lambda g, p=w: (g['close'] - g['low'].rolling(p, min_periods=1).min()) / (g['high'].rolling(p, min_periods=1).max() - g['low'].rolling(p, min_periods=1).min() + 1e-8)).reset_index(level=0, drop=True)\n",
    "        df[f'stoch_{w}'] = k\n",
    "        df[f'stoch_signal_{w}'] = df.groupby('ticker')[f'stoch_{w}'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "\n",
    "    # ADX and DI\n",
    "    def _compute_adx(g, period=14):\n",
    "        high = g['high']; low = g['low']; close = g['close']\n",
    "        prev_high = high.shift(1); prev_low = low.shift(1); prev_close = close.shift(1)\n",
    "        plus_dm = (high - prev_high).where((high - prev_high) > (prev_low - low), 0.0)\n",
    "        minus_dm = (prev_low - low).where((prev_low - low) > (high - prev_high), 0.0)\n",
    "        tr = pd.concat([high - low, (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=period, min_periods=1).mean()\n",
    "        plus_di = 100 * (plus_dm.rolling(window=period, min_periods=1).mean() / (atr + 1e-8))\n",
    "        minus_di = 100 * (minus_dm.rolling(window=period, min_periods=1).mean() / (atr + 1e-8))\n",
    "        dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di + 1e-8)\n",
    "        adx = dx.rolling(window=period, min_periods=1).mean()\n",
    "        return adx, plus_di, minus_di\n",
    "\n",
    "    for w in [10, 14, 20]:\n",
    "        if use_fi:\n",
    "            try:\n",
    "                df[f'adx_{w}'] = fi.adx(df['high'], df['low'], df['close'], window=w)\n",
    "                df[f'adx_pos_{w}'] = fi.adx_pos(df['high'], df['low'], df['close'], window=w)\n",
    "                df[f'adx_neg_{w}'] = fi.adx_neg(df['high'], df['low'], df['close'], window=w)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        tmp = df.groupby('ticker').apply(lambda g, p=w: _compute_adx(g, period=p)).reset_index(level=0, drop=True)\n",
    "        # tmp is tuple series; unpack\n",
    "        df[f'adx_{w}'] = tmp.apply(lambda x: x[0])\n",
    "        df[f'adx_pos_{w}'] = tmp.apply(lambda x: x[1])\n",
    "        df[f'adx_neg_{w}'] = tmp.apply(lambda x: x[2])\n",
    "\n",
    "    # Ichimoku (basic components)\n",
    "    def _ichimoku(g, p1=9, p2=26, p3=52):\n",
    "        conv = (g['high'].rolling(p1, min_periods=1).max() + g['low'].rolling(p1, min_periods=1).min()) / 2\n",
    "        base = (g['high'].rolling(p2, min_periods=1).max() + g['low'].rolling(p2, min_periods=1).min()) / 2\n",
    "        span_a = ((conv + base) / 2).shift(p2)\n",
    "        span_b = ((g['high'].rolling(p3, min_periods=1).max() + g['low'].rolling(p3, min_periods=1).min()) / 2).shift(p2)\n",
    "        return conv, base, span_a, span_b\n",
    "\n",
    "    def _ichimoku(g, p1=9, p2=26, p3=52):\n",
    "        conv = (g['high'].rolling(p1, min_periods=1).max() + g['low'].rolling(p1, min_periods=1).min()) / 2\n",
    "        base = (g['high'].rolling(p2, min_periods=1).max() + g['low'].rolling(p2, min_periods=1).min()) / 2\n",
    "        span_a = ((conv + base) / 2).shift(p2)\n",
    "        span_b = ((g['high'].rolling(p3, min_periods=1).max() + g['low'].rolling(p3, min_periods=1).min()) / 2).shift(p2)\n",
    "        return conv, base, span_a, span_b\n",
    "\n",
    "    # Prefer vendor functions; if they fail, compute per-group and assign to preserve indices\n",
    "    if use_fi:\n",
    "        try:\n",
    "            df['senkou_span_a'] = fi.ichimoku_a(df['high'], df['low'], df['close'], window1=9, window2=26, window3=52)\n",
    "            df['senkou_span_b'] = fi.ichimoku_b(df['high'], df['low'], df['close'], window1=9, window2=26, window3=52)\n",
    "            df['kijun_sen'] = fi.ichimoku_base_line(df['high'], df['low'], df['close'], window1=9, window2=26, window3=52)\n",
    "            df['tenkan_sen'] = fi.ichimoku_conversion_line(df['high'], df['low'], df['close'], window1=9, window2=26, window3=52)\n",
    "        except Exception:\n",
    "            use_fi = False\n",
    "\n",
    "    if not use_fi:\n",
    "        # compute per-ticker and assign by index to avoid misalignment\n",
    "        df['tenkan_sen'] = np.nan\n",
    "        df['kijun_sen'] = np.nan\n",
    "        df['senkou_span_a'] = np.nan\n",
    "        df['senkou_span_b'] = np.nan\n",
    "        for t, g in df.groupby('ticker'):\n",
    "            conv, base, span_a, span_b = _ichimoku(g)\n",
    "            idx = g.index\n",
    "            df.loc[idx, 'tenkan_sen'] = conv.values\n",
    "            df.loc[idx, 'kijun_sen'] = base.values\n",
    "            df.loc[idx, 'senkou_span_a'] = span_a.values\n",
    "            df.loc[idx, 'senkou_span_b'] = span_b.values\n",
    "    # Volume indicators\n",
    "    if use_fi:\n",
    "        try:\n",
    "            df['obv'] = fi.obv(df['close'], df['volume'])\n",
    "        except Exception:\n",
    "            df['obv'] = df.groupby('ticker').apply(lambda g: (np.sign(g['close'].diff()) * g['volume']).fillna(0).cumsum()).reset_index(level=0, drop=True)\n",
    "        try:\n",
    "            df['mfi_14'] = fi.mfi(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "        except Exception:\n",
    "            # fallback simple MFI approximation\n",
    "            def _mfi(g, period=14):\n",
    "                typical = (g['high'] + g['low'] + g['close']) / 3.0\n",
    "                money = typical * g['volume']\n",
    "                pos = (typical > typical.shift(1)).astype(int)\n",
    "                mf_pos = money.where(pos==1, 0).rolling(window=period, min_periods=1).sum()\n",
    "                mf_neg = money.where(pos==0, 0).rolling(window=period, min_periods=1).sum()\n",
    "                mfr = mf_pos / (mf_neg + 1e-8)\n",
    "                return 100 - (100 / (1 + mfr))\n",
    "            df['mfi_14'] = df.groupby('ticker').apply(lambda g: _mfi(g)).reset_index(level=0, drop=True)\n",
    "        try:\n",
    "            df['vwap_14'] = fi.vwap(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "        except Exception:\n",
    "            df['vwap_14'] = df.groupby('ticker').apply(lambda g: (g['close'] * g['volume']).rolling(window=14, min_periods=1).sum() / (g['volume'].rolling(window=14, min_periods=1).sum() + 1e-8)).reset_index(level=0, drop=True)\n",
    "    else:\n",
    "        df['obv'] = df.groupby('ticker').apply(lambda g: (np.sign(g['close'].diff()) * g['volume']).fillna(0).cumsum()).reset_index(level=0, drop=True)\n",
    "        def _mfi(g, period=14):\n",
    "            typical = (g['high'] + g['low'] + g['close']) / 3.0\n",
    "            money = typical * g['volume']\n",
    "            pos = (typical > typical.shift(1)).astype(int)\n",
    "            mf_pos = money.where(pos==1, 0).rolling(window=period, min_periods=1).sum()\n",
    "            mf_neg = money.where(pos==0, 0).rolling(window=period, min_periods=1).sum()\n",
    "            mfr = mf_pos / (mf_neg + 1e-8)\n",
    "            return 100 - (100 / (1 + mfr))\n",
    "        df['mfi_14'] = df.groupby('ticker').apply(lambda g: _mfi(g)).reset_index(level=0, drop=True)\n",
    "        df['vwap_14'] = df.groupby('ticker').apply(lambda g: (g['close'] * g['volume']).rolling(window=14, min_periods=1).sum() / (g['volume'].rolling(window=14, min_periods=1).sum() + 1e-8)).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Advanced / vendor-specific indicators: prefer fi.* implementations when available\n",
    "    adv_names = ['supertrend_14', 'zigzag', 'fvg', 'liquidity']\n",
    "    for name in adv_names:\n",
    "        if use_fi:\n",
    "            try:\n",
    "                if name == 'supertrend_14':\n",
    "                    df[name] = fi.supertrend(df['high'], df['low'], df['close'], window=14)\n",
    "                elif name == 'zigzag':\n",
    "                    df[name] = fi.zigzag(df['high'], df['low'], dev_threshold=5.0, depth=10)\n",
    "                elif name == 'fvg':\n",
    "                    df[name] = fi.fvg(df['open'], df['high'], df['low'], df['close'], join_consecutive=True)\n",
    "                elif name == 'liquidity':\n",
    "                    df[name] = fi.liquidity(df['open'], df['high'], df['low'], df['close'])\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallbacks: simple placeholders or NaN\n",
    "        if name == 'supertrend_14':\n",
    "            # simple supertrend-like: close - rolling median of ATR\n",
    "            df[name] = df[f'atr_14'] * 0.0\n",
    "        else:\n",
    "            df[name] = np.nan\n",
    "\n",
    "    # Ensure volume exists\n",
    "    if 'volume' not in df.columns:\n",
    "        df['volume'] = 0.0\n",
    "\n",
    "    # per-ticker stats used for embedding/score\n",
    "    if 'ticker' in df.columns:\n",
    "        df['ret'] = df.groupby('ticker')['close'].pct_change()\n",
    "        ticker_stats = df.groupby('ticker').agg(\n",
    "            mean_ret=('ret', 'mean'),\n",
    "            vol_ret=('ret', 'std'),\n",
    "            pct_pos=('ret', lambda x: (x > 0).mean()),\n",
    "            avg_vol=('volume', 'mean'),\n",
    "            count=('ret', 'count')\n",
    "        ).reset_index()\n",
    "        ticker_stats['sharpe_like'] = ticker_stats['mean_ret'] / ticker_stats['vol_ret'].replace(0, np.nan).fillna(1e-6)\n",
    "        _sc = MinMaxScaler()\n",
    "        ticker_stats['ticker_score'] = _sc.fit_transform(ticker_stats[['sharpe_like']].fillna(0))\n",
    "        df = df.merge(ticker_stats[['ticker', 'ticker_score']], on='ticker', how='left')\n",
    "        ticker_to_id = {t: i for i, t in enumerate(sorted(df['ticker'].unique()))}\n",
    "        df['ticker_id'] = df['ticker'].map(ticker_to_id)\n",
    "\n",
    "    # --- New Feature Engineering Section ---\n",
    "    print(\"Generating advanced features...\")\n",
    "    \n",
    "    # a. Stationarity Features (Rolling Z-Scores)\n",
    "    z_score_window = 30\n",
    "    indicators_to_zscore = ['rsi_14', f'macd_{macd_params[0][0]}_{macd_params[0][1]}_{macd_params[0][2]}', 'stoch_14', 'mfi_14']\n",
    "    \n",
    "    # FIX: Ensure columns are numeric before performing rolling calculations\n",
    "    for indicator in indicators_to_zscore:\n",
    "        if indicator in df.columns:\n",
    "            df[indicator] = pd.to_numeric(df[indicator], errors='coerce')\n",
    "\n",
    "    for indicator in indicators_to_zscore:\n",
    "        if indicator in df.columns:\n",
    "            rolling_mean = df.groupby('ticker')[indicator].transform(lambda x: x.rolling(window=z_score_window).mean())\n",
    "            rolling_std = df.groupby('ticker')[indicator].transform(lambda x: x.rolling(window=z_score_window).std())\n",
    "            df[f'{indicator}_zscore'] = (df[indicator] - rolling_mean) / (rolling_std + 1e-8)\n",
    "\n",
    "    # b. Relational Features\n",
    "    if 'ema_20' in df.columns and 'ema_5' in df.columns:\n",
    "        df['price_vs_ema20'] = df['close'] / df['ema_20']\n",
    "        df['ema5_vs_ema20'] = df['ema_5'] / df['ema_20']\n",
    "    \n",
    "    if 'bollinger_lband_20_2.0' in df.columns and 'bollinger_hband_20_2.0' in df.columns:\n",
    "        df['price_in_bb'] = (df['close'] - df['bollinger_lband_20_2.0']) / (df['bollinger_hband_20_2.0'] - df['bollinger_lband_20_2.0'] + 1e-8)\n",
    "\n",
    "    # c. Market Regime Features\n",
    "    if 'atr_14' in df.columns:\n",
    "        df['normalized_atr_14'] = df['atr_14'] / df['close']\n",
    "    \n",
    "    if 'ret' in df.columns:\n",
    "        df['volatility_60d'] = df.groupby('ticker')['ret'].transform(lambda x: x.rolling(60).std())\n",
    "        \n",
    "    if 'adx_14' in df.columns:\n",
    "        df['is_trending'] = (df['adx_14'] > 25).astype(int)\n",
    "        \n",
    "    print(\"Advanced features generated.\")\n",
    "    # --- End of New Feature Engineering Section ---\n",
    "\n",
    "    # --- Start of Super Advanced Feature Engineering ---\n",
    "    print(\"Generating interaction, confirmation, and RoC features...\")\n",
    "\n",
    "    # a. Interaction Features\n",
    "    if 'rsi_14' in df.columns and 'adx_14' in df.columns:\n",
    "        df['rsi_x_adx14'] = df['rsi_14'] * df['adx_14']\n",
    "\n",
    "    if 'volume' in df.columns and 'volatility_60d' in df.columns:\n",
    "        # Normalize volume before multiplying\n",
    "        df['volume_norm'] = df.groupby('ticker')['volume'].transform(lambda x: (x - x.mean()) / (x.std() + 1e-8))\n",
    "        df['vol_x_volatility'] = df['volume_norm'] * df['volatility_60d']\n",
    "\n",
    "    # b. Confirmation Features\n",
    "    # Ensure all necessary columns exist before creating the list\n",
    "    bullish_conditions = []\n",
    "    if 'ema_50' in df.columns:\n",
    "        bullish_conditions.append(df['close'] > df['ema_50'])\n",
    "    if f'macd_diff_12_26_9' in df.columns:\n",
    "        bullish_conditions.append(df[f'macd_diff_12_26_9'] > 0)\n",
    "    if 'rsi_14' in df.columns:\n",
    "        bullish_conditions.append(df['rsi_14'] > 50)\n",
    "    if 'adx_pos_14' in df.columns and 'adx_neg_14' in df.columns:\n",
    "        bullish_conditions.append(df['adx_pos_14'] > df['adx_neg_14'])\n",
    "    \n",
    "    if bullish_conditions:\n",
    "        df['bullish_confirmation_score'] = np.sum(bullish_conditions, axis=0)\n",
    "\n",
    "    # c. Rate of Change (RoC) Features\n",
    "    if 'ema_5' in df.columns:\n",
    "        df['ema_5_roc_10'] = df.groupby('ticker')['ema_5'].transform(lambda x: x.pct_change(10))\n",
    "    if 'rsi_14' in df.columns:\n",
    "        df['rsi_14_roc_5'] = df.groupby('ticker')['rsi_14'].transform(lambda x: x.pct_change(5))\n",
    "        \n",
    "    print(\"Super advanced features generated.\")\n",
    "    # --- End of Super Advanced Feature Engineering ---\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a05d9-dcd1-413e-84d0-8e3218d6bed6",
   "metadata": {},
   "source": [
    "Hàm `build_windows_multi_ticker()` thực hiện việc chia dữ liệu thành các *windows* thời gian cho mỗi cổ phiếu (ticker), chuẩn bị dữ liệu cho mô hình học máy. Cụ thể, nó tạo các chuỗi con (subsequences) của dữ liệu và nhãn `labels` từ dữ liệu chuỗi thời gian, để có thể huấn luyện các mô hình học sâu (deep learning models) như LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5dec65f-471c-4f8b-a9b2-9da1c432f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows_multi_ticker(df, feature_cols, window=30, step=1):\n",
    "    all_X, all_y, all_tids, all_times, all_open, all_close, all_prev_close = [], [], [], [], [], [], []\n",
    "    ticker_to_id = {t: i for i, t in enumerate(sorted(df['ticker'].unique()))}\n",
    "    for ticker, g in df.groupby('ticker'):\n",
    "        g = g.sort_values('timestamp').reset_index(drop=True)\n",
    "        data = g[feature_cols].values\n",
    "        targets = g['target'].values\n",
    "        opens = g['open'].values\n",
    "        closes = g['close'].values\n",
    "        n = len(g)\n",
    "        if n <= window:\n",
    "            continue\n",
    "        Xs, ys, times, opens_s, closes_s, prev_closes = [], [], [], [], [], []\n",
    "        for start in range(0, n-window, step):\n",
    "            end = start + window\n",
    "            Xs.append(data[start:end])\n",
    "            ys.append(targets[end])\n",
    "            times.append(g.loc[end, 'timestamp'])\n",
    "            opens_s.append(opens[end])\n",
    "            closes_s.append(closes[end])\n",
    "            prev_closes.append(closes[end-1])\n",
    "        if len(ys) == 0:\n",
    "            continue\n",
    "        all_X.append(np.array(Xs))\n",
    "        all_y.append(np.array(ys))\n",
    "        all_tids.append(np.full(len(ys), ticker_to_id[ticker], dtype=int))\n",
    "        all_times.append(np.array(times))\n",
    "        all_open.append(np.array(opens_s))\n",
    "        all_close.append(np.array(closes_s))\n",
    "        all_prev_close.append(np.array(prev_closes))\n",
    "    if len(all_X) == 0:\n",
    "        return None\n",
    "    X_all = np.vstack(all_X)\n",
    "    y_all = np.concatenate(all_y)\n",
    "    tid_all = np.concatenate(all_tids)\n",
    "    times_all = np.concatenate(all_times)\n",
    "    opens_all = np.concatenate(all_open)\n",
    "    closes_all = np.concatenate(all_close)\n",
    "    prev_all = np.concatenate(all_prev_close)\n",
    "    # sort by times to preserve chronology across tickers\n",
    "    order = np.argsort(times_all)\n",
    "    return X_all[order], y_all[order], tid_all[order], times_all[order], opens_all[order], closes_all[order], prev_all[order], ticker_to_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b88974-4a05-4544-8504-1ad1d8a59c24",
   "metadata": {},
   "source": [
    "Hàm `build_model()` định nghĩa một mô hình học sâu (deep learning model) sử dụng các lớp **Conv1D**, **LSTM**, **Multi-Head Attention**, và **Embedding** để xử lý chuỗi thời gian với thông tin bổ sung từ cổ phiếu (tickers). Mô hình này phù hợp cho các bài toán phân loại nhị phân, chẳng hạn như dự đoán xu hướng của cổ phiếu (tăng/giảm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01e15f51-d2cd-43a3-9274-9738f28654f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(window, n_features, n_tickers, emb_dim=8,\n",
    "                lstm_units=128, conv_filters=32, att_heads=4, att_key_dim=32, dropout=0.5):\n",
    "    seq_in = Input(shape=(window, n_features), name='seq')\n",
    "    tick_in = Input(shape=(), dtype='int32', name='ticker')\n",
    "    \n",
    "    emb = Embedding(input_dim=n_tickers, output_dim=emb_dim, name='emb')(tick_in)\n",
    "    emb = Flatten()(emb)\n",
    "    \n",
    "    x = Conv1D(filters=conv_filters, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.001))(seq_in)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = LSTM(lstm_units, return_sequences=True, kernel_regularizer=l2(0.001))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    mha = MultiHeadAttention(num_heads=att_heads, key_dim=att_key_dim)(x, x)\n",
    "    x = LayerNormalization()(x + mha)\n",
    "    \n",
    "    gap = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    merged = Concatenate()([gap, emb])\n",
    "    d = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(merged)\n",
    "    d = Dropout(dropout)(d)\n",
    "    \n",
    "    # THAY ĐỔI LẠI: 1 unit và activation 'sigmoid' cho phân loại nhị phân\n",
    "    out = Dense(1, activation='sigmoid')(d)\n",
    "    \n",
    "    model = Model(inputs=[seq_in, tick_in], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc0e62-76c5-4232-8aed-712db5c4296c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h2>LUỒNG CHẠY</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480befd9-669c-42c3-93f6-027bff34013f",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <i>Quy trình đầy đủ: tải dữ liệu, tính toán các chỉ số kỹ thuật, huấn luyện mô hình, cho đến việc đánh giá và đưa ra các tín hiệu mua bán tiềm năng dựa trên các dự đoán của mô hình. Mục tiêu của quy trình này là phát hiện các tín hiệu tiềm năng trong tập dữ liệu thử nghiệm và tạo các báo cáo liên quan đến mô hình.</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52c6c582-aa12-48c2-aab8-340f48fd936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Force GPU context\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        print(\"Starting pipeline with device:\", tf.config.list_physical_devices('GPU')[0] if tf.config.list_physical_devices('GPU') else 'CPU')\n",
    "        \n",
    "        print(\"Loading data...\", CSV_PATH)\n",
    "        df = load_data(CSV_PATH)\n",
    "        check_memory()\n",
    "        \n",
    "        # Reduce data size if needed\n",
    "        unique_tickers = df['ticker'].nunique()\n",
    "        if unique_tickers > 20:  # Limit to 20 tickers for demo\n",
    "            top_tickers = df['ticker'].value_counts().head(20).index\n",
    "            df = df[df['ticker'].isin(top_tickers)].reset_index(drop=True)\n",
    "            print(f\"Limited to top {len(top_tickers)} tickers\")\n",
    "        \n",
    "        # compute technical indicators used later in feature selection\n",
    "        print(\"Computing indicators...\")\n",
    "        df = compute_indicators(df)\n",
    "        check_memory()\n",
    "        \n",
    "        print(\"Rows,cols:\", df.shape)\n",
    "        if START_DATE is not None or END_DATE is not None:\n",
    "            if START_DATE is not None:\n",
    "                df = df[df['timestamp'] >= pd.to_datetime(START_DATE)]\n",
    "            if END_DATE is not None:\n",
    "                df = df[df['timestamp'] <= pd.to_datetime(END_DATE)]\n",
    "            df = df.reset_index(drop=True)\n",
    "            print(\"Filtered date range ->\", df['timestamp'].min(), df['timestamp'].max())\n",
    "\n",
    "        # drop cols with many missing\n",
    "        df = drop_high_missing(df, MAX_MISSING_RATIO)\n",
    "\n",
    "        # numeric cols for interpolation/selection\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # ensure 'target' etc not in numericcols yet\n",
    "\n",
    "        # interpolate per ticker\n",
    "        df = interp_per_ticker(df, numeric_cols)\n",
    "\n",
    "        # build target using Triple Barrier Method\n",
    "        # Ensure 'atr_14' is computed in compute_indicators\n",
    "        df['target'] = apply_triple_barrier(df, \n",
    "                                            tp_mult=TAKE_PROFIT_MULT, \n",
    "                                            sl_mult=STOP_LOSS_MULT, \n",
    "                                            max_period=MAX_HOLD_PERIOD)\n",
    "        # Drop rows where a label could not be assigned\n",
    "        df = df.dropna(subset=['target']).copy()\n",
    "        df['target'] = df['target'].astype(int)\n",
    "\n",
    "        # ===============================================\n",
    "        # ======= THÊM DÒNG CODE MỚI VÀO ĐÂY =======\n",
    "        print(f\"Original data size before filtering timeouts: {len(df)}\")\n",
    "        df = df[df['target'] != 2].copy()\n",
    "        print(f\"Data size after filtering timeouts (target=2): {len(df)}\")\n",
    "        # ===============================================\n",
    "\n",
    "        # time split mask for train-only selection\n",
    "        train_mask = time_split_train_mask(df, train_frac=0.70)\n",
    "\n",
    "        # numeric columns used in RFE: exclude target, ret, rf_daily\n",
    "        num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in ['target','ret','rf_daily']]\n",
    "        print(\"Numeric cols available for selection:\", len(num_cols))\n",
    "\n",
    "        # train-only RFE\n",
    "        selected_rfe, rfe_obj = train_only_rfe(df, num_cols, train_mask, n_select=N_RFE_FEATURES, n_estimators=RFE_ESTIMATORS)\n",
    "        print(\"RFE selected:\", selected_rfe)\n",
    "\n",
    "        # # KMeans on selected features (train rows only)\n",
    "        # X_train_sel = df.loc[train_mask, selected_rfe]\n",
    "        # labels_cluster, scaler_k, km_obj = kmeans_cluster_features(X_train_sel, n_clusters=N_CLUSTERS)\n",
    "\n",
    "        # # SOM per cluster selection to get ~FINAL_FEATURE_COUNT features\n",
    "        # selected_final = som_select_per_cluster(X_train_sel, labels_cluster, selected_rfe, target_count=FINAL_FEATURE_COUNT)\n",
    "        # print(\"SOM selected (pre-corr):\", selected_final)\n",
    "        \n",
    "        selected_final = selected_rfe \n",
    "        # correlation filter\n",
    "        final_feats = correlation_filter(df.loc[train_mask], selected_final, threshold=0.8)\n",
    "        if len(final_feats) > FINAL_FEATURE_COUNT:\n",
    "            final_feats = final_feats[:FINAL_FEATURE_COUNT]\n",
    "        print(\"Final features after corr filtering:\", final_feats)\n",
    "\n",
    "        # ensure we have features\n",
    "        if len(final_feats) == 0:\n",
    "            raise RuntimeError(\"No features selected - check data and parameters\")\n",
    "        n_features = len(final_feats)\n",
    "                # Chuẩn hóa dữ liệu theo từng Ticker (Per-Ticker Scaling)\n",
    "        # Chúng ta sẽ fit scaler trên dữ liệu training của mỗi ticker và transform cho toàn bộ dữ liệu của ticker đó.\n",
    "        print(\"Performing per-ticker scaling...\")\n",
    "        df_scaled = df.copy() # Tạo bản sao để chứa dữ liệu đã chuẩn hóa\n",
    "        for ticker, group in df_scaled.groupby('ticker'):\n",
    "            # Tạo một scaler riêng cho mỗi ticker\n",
    "            scaler = StandardScaler()\n",
    "            \n",
    "            # Xác định dữ liệu training cho ticker này\n",
    "            train_data_ticker = group.loc[train_mask[df['ticker'] == ticker], final_feats]\n",
    "            \n",
    "            # Fit scaler CHỈ trên dữ liệu training của ticker\n",
    "            if not train_data_ticker.empty:\n",
    "                scaler.fit(train_data_ticker)\n",
    "                \n",
    "                # Áp dụng scaler đã fit để transform toàn bộ dữ liệu của ticker đó\n",
    "                group_scaled_values = scaler.transform(group[final_feats])\n",
    "                \n",
    "                # Gán lại giá trị đã chuẩn hóa vào DataFrame\n",
    "                df_scaled.loc[group.index, final_feats] = group_scaled_values\n",
    "\n",
    "        print(\"Per-ticker scaling complete.\")\n",
    "        # build multi-ticker windows (also returns ticker mapping)\n",
    "        res = build_windows_multi_ticker(df_scaled, final_feats, window=WINDOW, step=STEP)\n",
    "        if res is None:\n",
    "            raise RuntimeError('No windows created (data too short for given window)')\n",
    "        X_all, y_all, tids_all, times_all, opens_all, closes_all, prev_all, ticker_to_id = res\n",
    "        n_total = X_all.shape[0]\n",
    "        print(f\"Built {n_total} samples across {len(ticker_to_id)} tickers\")\n",
    "\n",
    "        # split by chronological order\n",
    "        train_end = int(n_total * 0.70)\n",
    "        val_end = int(n_total * 0.85)\n",
    "        X_train, y_train, t_train = X_all[:train_end], y_all[:train_end], tids_all[:train_end]\n",
    "        X_val, y_val, t_val = X_all[train_end:val_end], y_all[train_end:val_end], tids_all[train_end:val_end]\n",
    "        X_test, y_test, t_test = X_all[val_end:], y_all[val_end:], tids_all[val_end:]\n",
    "        times_test = times_all[val_end:] \n",
    "        opens_test = opens_all[val_end:]; closes_test = closes_all[val_end:]; prev_test = prev_all[val_end:]\n",
    "        print(\"Split shapes\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "        # build model\n",
    "        n_tickers = len(ticker_to_id)\n",
    "        model = build_model(WINDOW, n_features, n_tickers, emb_dim=EMB_DIM)\n",
    "        model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        # Tăng trọng số cho lớp 1 (win) lên, ví dụ gấp 2.5 lần so với lớp 0\n",
    "        n0 = (y_train == 0).sum()\n",
    "        n1 = (y_train == 1).sum()\n",
    "        class_weight_dict = {0: 1.0, 1: 2 * (n0 / n1) if n1 > 0 else 1.0} \n",
    "        print('class_weight used in fit (manual adjustment):', class_weight_dict)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "        ckpt = ModelCheckpoint(os.path.join(OUT_DIR, 'best_model.h5'), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "        # Train with GPU monitoring\n",
    "        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "            history = model.fit(\n",
    "                [X_train, t_train], y_train,\n",
    "                validation_data=([X_val, t_val], y_val),\n",
    "                epochs=EPOCHS, \n",
    "                batch_size=BATCH_SIZE,\n",
    "                class_weight=class_weight_dict,\n",
    "                callbacks=[es, ckpt], \n",
    "                verbose=2\n",
    "            )\n",
    "\n",
    "        # save training plots\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(history.history.get('loss', []), label='train_loss')\n",
    "        plt.plot(history.history.get('val_loss', []), label='val_loss')\n",
    "        plt.legend(); plt.title('Loss'); plt.savefig(os.path.join(OUT_DIR, 'loss.png'))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(history.history.get('accuracy', []), label='train_accuracy')\n",
    "        plt.plot(history.history.get('val_accuracy', []), label='val_accuracy')\n",
    "        plt.legend(); plt.title('Accuracy'); plt.savefig(os.path.join(OUT_DIR, 'accuracy.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # predict and evaluate\n",
    "        y_prob = model.predict([X_test, t_test]).ravel() # .ravel() để làm phẳng array\n",
    "        y_val_prob = model.predict([X_val, t_val]).ravel()\n",
    "        \n",
    "        best_threshold = 0.75 # Đặt thẳng ngưỡng\n",
    "        print(f\"\\nUsing fixed high threshold: {best_threshold:.2f}\")\n",
    "\n",
    "        # Áp dụng ngưỡng tối ưu lên tập test\n",
    "        y_pred = (y_prob >= best_threshold).astype(int)\n",
    "        \n",
    "        # ==================================================================\n",
    "        # ======= BẮT ĐẦU CODE MỚI: TÌM VÀ IN CÁC CỔ PHIẾU TIỀM NĂNG =======\n",
    "\n",
    "        # Tạo một dictionary để map từ ID về lại tên ticker\n",
    "        id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
    "\n",
    "        # Tạo một DataFrame để tổng hợp kết quả trên tập test\n",
    "        results_df = pd.DataFrame({\n",
    "            'timestamp': times_test,\n",
    "            'ticker_id': t_test,\n",
    "            'probability': y_prob, # Xác suất thô từ mô hình\n",
    "            'prediction': y_pred  # Dự đoán cuối cùng (0 hoặc 1)\n",
    "        })\n",
    "        results_df['ticker'] = results_df['ticker_id'].map(id_to_ticker)\n",
    "\n",
    "        # Lọc ra những tín hiệu được dự đoán là \"win\" (tiềm năng)\n",
    "        potential_signals = results_df[results_df['prediction'] == 1].copy()\n",
    "        potential_signals = potential_signals.sort_values(by='timestamp')\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"       tín hiệu mua tiềm năng được phát hiện trên tập test\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        if potential_signals.empty:\n",
    "            print(\"Không tìm thấy tín hiệu nào.\")\n",
    "        else:\n",
    "            # In ra kết quả\n",
    "            print(potential_signals[['timestamp', 'ticker', 'probability']].to_string(index=False))\n",
    "\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # ======= KẾT THÚC CODE MỚI =======\n",
    "        # ==================================================================\n",
    "        \n",
    "        print(\"\\nClassification Report on Test Set using Optimal Threshold:\")\n",
    "\n",
    "        # HIỂN THỊ LẠI BÁO CÁO CHO 2 LỚP\n",
    "        print(classification_report(y_test, y_pred, target_names=['loss', 'win']))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print('Confusion matrix:\\n', cm)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=['loss', 'win'], yticklabels=['loss', 'win']); plt.title('Confusion Matrix'); plt.savefig(os.path.join(OUT_DIR, 'confusion_matrix.png')); plt.close()\n",
    "\n",
    "        # per-ticker AUC is not directly applicable for multi-class, skipping\n",
    "        \n",
    "        # simple backtests on test set using opens/closes stored earlier\n",
    "        # Only consider 'win' predictions (label 1) for long positions\n",
    "        profits_tf = np.where(y_pred == 1, closes_test - prev_test, 0.0)\n",
    "        profits_ls = np.where(y_pred == 1, closes_test - opens_test, 0.0) # Simplified for long-only on win signal\n",
    "        print('Trend-following cum profit (long on \"win\" signal):', np.nansum(profits_tf))\n",
    "        print('Long-short cum profit (long on \"win\" signal):', np.nansum(profits_ls))\n",
    "\n",
    "        model.save(os.path.join(OUT_DIR, 'model_final.h5'))\n",
    "        print('Done. Outputs saved to', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f2c8df6-6c41-4e93-b57d-48c06a8ba869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline with device: CPU\n",
      "Loading data... DSTC_3y.csv\n",
      "Loading data in chunks...\n",
      "Memory usage: 80.1% (12.6/15.7 GB)\n",
      "Memory usage: 80.1% (12.6/15.7 GB)\n",
      "Memory usage: 80.2% (12.6/15.7 GB)\n",
      "Data loaded: (22410, 7)\n",
      "Memory usage: 80.4% (12.6/15.7 GB)\n",
      "Limited to top 20 tickers\n",
      "Computing indicators...\n",
      "Generating advanced features...\n",
      "Advanced features generated.\n",
      "Generating interaction, confirmation, and RoC features...\n",
      "Super advanced features generated.\n",
      "Memory usage: 64.7% (10.2/15.7 GB)\n",
      "Rows,cols: (14940, 97)\n",
      "Dropping 3 columns with >10.0% missing\n",
      "Applying Triple-Barrier with TP=2*ATR, SL=1*ATR, Max Hold=7 days...\n",
      "Original data size before filtering timeouts: 14920\n",
      "Data size after filtering timeouts (target=2): 10830\n",
      "Time-split train_end date: 2024-10-04T00:00:00.000000000 -> train rows: 7532 / 10830\n",
      "Numeric cols available for selection: 91\n",
      "Fitting RFE on train rows... (this may take time)\n",
      "\n",
      "Feature Ranking by RFE:\n",
      "Rank 1: adx_10\n",
      "Rank 1: adx_14\n",
      "Rank 1: adx_20\n",
      "Rank 1: adx_neg_10\n",
      "Rank 1: adx_pos_10\n",
      "Rank 1: adx_pos_14\n",
      "Rank 1: adx_pos_20\n",
      "Rank 1: atr_14\n",
      "Rank 1: atr_21\n",
      "Rank 1: atr_7\n",
      "Rank 1: bollinger_hband_30_1.5\n",
      "Rank 1: bollinger_hband_30_2.0\n",
      "Rank 1: bollinger_hband_30_2.5\n",
      "Rank 1: bollinger_lband_10_1.5\n",
      "Rank 1: bollinger_lband_30_1.5\n",
      "Rank 1: bollinger_lband_30_2.5\n",
      "Rank 1: ema5_vs_ema20\n",
      "Rank 1: ema_50\n",
      "Rank 1: ema_5_roc_10\n",
      "Rank 1: macd_12_26_9\n",
      "Rank 1: macd_12_26_9_zscore\n",
      "Rank 1: macd_20_50_10\n",
      "Rank 1: macd_diff_12_26_9\n",
      "Rank 1: macd_diff_20_50_10\n",
      "Rank 1: macd_signal_12_26_9\n",
      "Rank 1: macd_signal_20_50_10\n",
      "Rank 1: mfi_14\n",
      "Rank 1: mfi_14_zscore\n",
      "Rank 1: normalized_atr_14\n",
      "Rank 1: obv\n",
      "Rank 1: rsi_14_roc_5\n",
      "Rank 1: rsi_30\n",
      "Rank 1: rsi_x_adx14\n",
      "Rank 1: senkou_span_a\n",
      "Rank 1: senkou_span_b\n",
      "Rank 1: sma_50\n",
      "Rank 1: stoch_14_zscore\n",
      "Rank 1: volatility_60d\n",
      "Rank 1: volume\n",
      "Rank 1: wma_50\n",
      "Rank 2: rsi_14_zscore\n",
      "Rank 3: stoch_signal_20\n",
      "Rank 4: macd_signal_5_35_5\n",
      "Rank 5: macd_diff_5_35_5\n",
      "Rank 6: vol_x_volatility\n",
      "Rank 7: bollinger_hband_20_1.5\n",
      "Rank 8: bollinger_hband_20_2.5\n",
      "Rank 9: adx_neg_20\n",
      "Rank 10: bollinger_lband_20_2.5\n",
      "Rank 11: close\n",
      "Rank 12: stoch_signal_10\n",
      "Rank 13: rsi_14\n",
      "Rank 14: macd_5_35_5\n",
      "Rank 15: bollinger_lband_20_1.5\n",
      "Rank 16: volume_norm\n",
      "Rank 17: bollinger_lband_30_2.0\n",
      "Rank 18: adx_neg_14\n",
      "Rank 19: bollinger_lband_10_2.0\n",
      "Rank 20: rsi_7\n",
      "Rank 21: price_in_bb\n",
      "Rank 22: stoch_signal_14\n",
      "Rank 23: stoch_20\n",
      "Rank 24: price_vs_ema20\n",
      "Rank 25: bollinger_hband_20_2.0\n",
      "Rank 26: kijun_sen\n",
      "Rank 27: sma_20\n",
      "Rank 28: bollinger_lband_10_2.5\n",
      "Rank 29: stoch_14\n",
      "Rank 30: bollinger_lband_20_2.0\n",
      "Rank 31: vwap_14\n",
      "Rank 32: supertrend_14\n",
      "Rank 33: bollinger_hband_10_2.0\n",
      "Rank 34: wma_20\n",
      "Rank 35: stoch_10\n",
      "Rank 36: tenkan_sen\n",
      "Rank 37: high\n",
      "Rank 38: ema_20\n",
      "Rank 39: bollinger_hband_10_2.5\n",
      "Rank 40: ema_5\n",
      "Rank 41: ticker_score\n",
      "Rank 42: ema_10\n",
      "Rank 43: bollinger_hband_10_1.5\n",
      "Rank 44: wma_5\n",
      "Rank 45: wma_10\n",
      "Rank 46: sma_5\n",
      "Rank 47: ticker_id\n",
      "Rank 48: low\n",
      "Rank 49: open\n",
      "Rank 50: sma_10\n",
      "Rank 51: bullish_confirmation_score\n",
      "Rank 52: is_trending\n",
      "RFE selected: ['volume', 'ema_50', 'sma_50', 'wma_50', 'rsi_30', 'atr_7', 'atr_14', 'atr_21', 'bollinger_lband_10_1.5', 'bollinger_hband_30_1.5', 'bollinger_lband_30_1.5', 'bollinger_hband_30_2.0', 'bollinger_hband_30_2.5', 'bollinger_lband_30_2.5', 'macd_12_26_9', 'macd_signal_12_26_9', 'macd_diff_12_26_9', 'macd_20_50_10', 'macd_signal_20_50_10', 'macd_diff_20_50_10', 'adx_10', 'adx_pos_10', 'adx_neg_10', 'adx_14', 'adx_pos_14', 'adx_20', 'adx_pos_20', 'senkou_span_a', 'senkou_span_b', 'obv', 'mfi_14', 'macd_12_26_9_zscore', 'stoch_14_zscore', 'mfi_14_zscore', 'ema5_vs_ema20', 'normalized_atr_14', 'volatility_60d', 'rsi_x_adx14', 'ema_5_roc_10', 'rsi_14_roc_5']\n",
      "Final features after corr filtering: ['volume', 'ema_50', 'rsi_30', 'atr_7', 'macd_12_26_9', 'macd_diff_12_26_9', 'macd_diff_20_50_10', 'adx_10', 'adx_pos_10', 'adx_neg_10', 'obv', 'mfi_14', 'macd_12_26_9_zscore', 'stoch_14_zscore', 'mfi_14_zscore', 'normalized_atr_14', 'rsi_x_adx14', 'rsi_14_roc_5']\n",
      "Performing per-ticker scaling...\n",
      "Per-ticker scaling complete.\n",
      "Built 10030 samples across 20 tickers\n",
      "Split shapes (7021, 40, 18) (1504, 40, 18) (1505, 40, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ seq (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,760</span> │ seq[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,432</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          │                           │                 │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                               │                           │                 │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ticker (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ ticker[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                               │                           │                 │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,280</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ seq (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m18\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m1,760\u001b[0m │ seq[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m82,432\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m256\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                 │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m66,048\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          │                           │                 │ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                               │                           │                 │ multi_head_attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ticker (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m)                    │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ emb (\u001b[38;5;33mEmbedding\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                │             \u001b[38;5;34m320\u001b[0m │ ticker[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ layer_normalization_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ global_average_pooling1d[\u001b[38;5;34m…\u001b[0m │\n",
       "│                               │                           │                 │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │           \u001b[38;5;34m9,280\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │              \u001b[38;5;34m65\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,417</span> (626.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,417\u001b[0m (626.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,417</span> (626.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,417\u001b[0m (626.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight used in fit (manual adjustment): {0: 1.0, 1: np.float64(3.532702915681639)}\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 - 27s - 244ms/step - accuracy: 0.4336 - loss: 1.4237 - val_accuracy: 0.3903 - val_loss: 0.9837\n",
      "Epoch 2/100\n",
      "110/110 - 11s - 103ms/step - accuracy: 0.4005 - loss: 1.3837 - val_accuracy: 0.3624 - val_loss: 1.0043\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 - 11s - 102ms/step - accuracy: 0.4192 - loss: 1.3594 - val_accuracy: 0.4315 - val_loss: 0.9521\n",
      "Epoch 4/100\n",
      "110/110 - 12s - 113ms/step - accuracy: 0.4054 - loss: 1.3478 - val_accuracy: 0.4415 - val_loss: 0.9668\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 - 14s - 124ms/step - accuracy: 0.4260 - loss: 1.3406 - val_accuracy: 0.4335 - val_loss: 0.9498\n",
      "Epoch 6/100\n",
      "110/110 - 12s - 106ms/step - accuracy: 0.4256 - loss: 1.3344 - val_accuracy: 0.4388 - val_loss: 0.9673\n",
      "Epoch 7/100\n",
      "110/110 - 13s - 114ms/step - accuracy: 0.4394 - loss: 1.3236 - val_accuracy: 0.4362 - val_loss: 0.9500\n",
      "Epoch 8/100\n",
      "110/110 - 12s - 112ms/step - accuracy: 0.4346 - loss: 1.3133 - val_accuracy: 0.4156 - val_loss: 1.0049\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 - 13s - 116ms/step - accuracy: 0.4247 - loss: 1.3173 - val_accuracy: 0.4402 - val_loss: 0.9348\n",
      "Epoch 10/100\n",
      "110/110 - 11s - 101ms/step - accuracy: 0.4378 - loss: 1.3082 - val_accuracy: 0.4621 - val_loss: 0.9403\n",
      "Epoch 11/100\n",
      "110/110 - 12s - 105ms/step - accuracy: 0.4454 - loss: 1.3025 - val_accuracy: 0.4435 - val_loss: 0.9734\n",
      "Epoch 12/100\n",
      "110/110 - 11s - 104ms/step - accuracy: 0.4381 - loss: 1.3002 - val_accuracy: 0.4488 - val_loss: 0.9450\n",
      "Epoch 13/100\n",
      "110/110 - 11s - 98ms/step - accuracy: 0.4602 - loss: 1.2868 - val_accuracy: 0.4767 - val_loss: 0.9420\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 - 12s - 109ms/step - accuracy: 0.4562 - loss: 1.2855 - val_accuracy: 0.4880 - val_loss: 0.9244\n",
      "Epoch 15/100\n",
      "110/110 - 12s - 105ms/step - accuracy: 0.4529 - loss: 1.2831 - val_accuracy: 0.4515 - val_loss: 0.9927\n",
      "Epoch 16/100\n",
      "110/110 - 11s - 101ms/step - accuracy: 0.4619 - loss: 1.2773 - val_accuracy: 0.4721 - val_loss: 0.9264\n",
      "Epoch 17/100\n",
      "110/110 - 11s - 101ms/step - accuracy: 0.4669 - loss: 1.2725 - val_accuracy: 0.4754 - val_loss: 0.9359\n",
      "Epoch 18/100\n",
      "110/110 - 11s - 96ms/step - accuracy: 0.4653 - loss: 1.2630 - val_accuracy: 0.4820 - val_loss: 0.9274\n",
      "Epoch 19/100\n",
      "110/110 - 11s - 102ms/step - accuracy: 0.4713 - loss: 1.2552 - val_accuracy: 0.4707 - val_loss: 0.9863\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\n",
      "Using fixed high threshold: 0.75\n",
      "\n",
      "==================================================\n",
      "       tín hiệu mua tiềm năng được phát hiện trên tập test\n",
      "==================================================\n",
      " timestamp ticker  probability\n",
      "2025-04-01    GAS     0.846616\n",
      "2025-04-02    FPT     0.762145\n",
      "2025-04-02    GAS     0.856641\n",
      "2025-04-03    SAB     0.750126\n",
      "2025-04-03    FPT     0.779371\n",
      "2025-04-03    GAS     0.849611\n",
      "2025-04-04    GAS     0.856159\n",
      "2025-04-04    FPT     0.792199\n",
      "2025-04-04    SAB     0.750371\n",
      "2025-04-08    FPT     0.806545\n",
      "2025-04-08    GAS     0.835054\n",
      "2025-04-09    GAS     0.850563\n",
      "2025-04-09    FPT     0.800912\n",
      "2025-04-10    GAS     0.838825\n",
      "2025-04-10    FPT     0.802807\n",
      "2025-04-11    FPT     0.802499\n",
      "2025-04-11    GAS     0.830172\n",
      "2025-04-14    GAS     0.812307\n",
      "2025-04-14    FPT     0.810197\n",
      "2025-04-15    GAS     0.807839\n",
      "2025-04-15    FPT     0.813410\n",
      "2025-04-16    GAS     0.794780\n",
      "2025-04-17    FPT     0.814911\n",
      "2025-04-17    GAS     0.797887\n",
      "2025-04-18    GAS     0.780249\n",
      "2025-04-18    FPT     0.815535\n",
      "2025-04-21    GAS     0.765397\n",
      "2025-04-21    FPT     0.817124\n",
      "2025-04-23    GAS     0.751708\n",
      "2025-04-24    GAS     0.757833\n",
      "2025-04-24    FPT     0.810791\n",
      "2025-04-25    FPT     0.812977\n",
      "2025-04-29    FPT     0.807235\n",
      "2025-05-05    FPT     0.811020\n",
      "2025-05-06    FPT     0.804324\n",
      "2025-05-07    FPT     0.805791\n",
      "2025-05-08    FPT     0.803307\n",
      "2025-05-09    FPT     0.797406\n",
      "2025-05-12    FPT     0.787806\n",
      "2025-05-13    FPT     0.795033\n",
      "2025-05-14    FPT     0.792279\n",
      "2025-05-15    FPT     0.795047\n",
      "2025-05-16    FPT     0.792044\n",
      "2025-05-20    FPT     0.798983\n",
      "2025-05-28    FPT     0.785182\n",
      "2025-06-02    PLX     0.754403\n",
      "2025-06-03    PLX     0.777937\n",
      "2025-06-04    PLX     0.779786\n",
      "2025-06-04    BID     0.758282\n",
      "2025-06-05    PLX     0.782509\n",
      "2025-06-06    FPT     0.798373\n",
      "2025-06-06    PLX     0.773720\n",
      "2025-06-09    GAS     0.763388\n",
      "2025-06-09    PLX     0.777351\n",
      "2025-06-10    FPT     0.787676\n",
      "2025-06-10    GAS     0.755668\n",
      "2025-06-10    PLX     0.773019\n",
      "2025-06-11    BID     0.757648\n",
      "2025-06-11    GAS     0.752177\n",
      "2025-06-11    PLX     0.779462\n",
      "2025-06-12    PLX     0.776952\n",
      "2025-06-12    GAS     0.771825\n",
      "2025-06-12    FPT     0.784457\n",
      "2025-06-13    PLX     0.764374\n",
      "2025-06-13    GAS     0.784377\n",
      "2025-06-16    FPT     0.780398\n",
      "2025-06-16    PLX     0.753694\n",
      "2025-06-16    GAS     0.775532\n",
      "2025-06-17    GAS     0.794594\n",
      "2025-06-17    FPT     0.776628\n",
      "2025-06-18    GAS     0.788815\n",
      "2025-06-19    GAS     0.787922\n",
      "2025-06-20    GAS     0.780871\n",
      "2025-06-23    GAS     0.777409\n",
      "2025-06-24    GAS     0.771264\n",
      "2025-06-25    FPT     0.767965\n",
      "2025-06-26    FPT     0.772268\n",
      "2025-06-26    BID     0.775257\n",
      "2025-06-26    GAS     0.755390\n",
      "2025-06-27    BID     0.771470\n",
      "2025-06-27    FPT     0.763928\n",
      "2025-06-30    BID     0.777267\n",
      "2025-06-30    FPT     0.768523\n",
      "2025-07-01    BID     0.772659\n",
      "2025-07-01    FPT     0.768060\n",
      "2025-07-02    FPT     0.758180\n",
      "2025-07-02    BID     0.763857\n",
      "2025-07-03    FPT     0.755117\n",
      "2025-07-03    BID     0.768812\n",
      "2025-07-04    FPT     0.761924\n",
      "2025-07-04    BID     0.766352\n",
      "2025-07-07    BID     0.757621\n",
      "2025-07-08    FPT     0.752392\n",
      "==================================================\n",
      "\n",
      "\n",
      "Classification Report on Test Set using Optimal Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        loss       0.54      0.95      0.69       805\n",
      "         win       0.53      0.07      0.12       700\n",
      "\n",
      "    accuracy                           0.54      1505\n",
      "   macro avg       0.53      0.51      0.41      1505\n",
      "weighted avg       0.53      0.54      0.42      1505\n",
      "\n",
      "Confusion matrix:\n",
      " [[761  44]\n",
      " [651  49]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend-following cum profit (long on \"win\" signal): 3412.782800000001\n",
      "Long-short cum profit (long on \"win\" signal): 27079.372559999916\n",
      "Done. Outputs saved to outputs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
